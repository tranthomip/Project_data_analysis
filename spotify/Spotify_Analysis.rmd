---
title: "Spotify Track Analysis"
author: "Thom Tran"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background
### About business
Spotify transformed music listening forever when it launched in 2008. Discover, manage and
share over 80 million tracks, including more than 4 million podcast titles, for free, or upgrade to Spotify Premium to access exclusive features for music including improved sound quality and an on-demand, offline, and ad-free music listening experience. Today, Spotify is the world’s most popular audio streaming subscription service with 433m users, including 188m subscribers, across 183 markets.

### Tasks
Through spotify data set, i will find out how music changed over years, top ten artist, track on Spotify, correlation between variables and factors effect on track popularity.

### Data source

The data set used in this analyze come from [Spotify Data Visulization](https://www.kaggle.com/code/varunsaikanuri/spotify-data-visualization) on Kaggle.


## Data Cleaning

**Load necessary package**

```{r}
install.packages("tidyverse")
install.packages("lubridate")
install.packages("dplyr")
install.packages("corrplot")
library(tidyverse)
library(lubridate)
library(dplyr)
```

**Load data set**
```{r}
song <- read.csv("songs_normalize.csv")
```

**Check column name**
```{r}
colnames(song)
```

Explain variables:

* artist: Name of the Artist
* song: Name of the Track.
* duration_ms: Duration of the track in milliseconds.
* explicit: The lyrics or content of a song or a music video contain one or more of the criteria which could be considered offensive or unsuitable for children.
* year: Release Year of the track.
* popularity: The higher the value the more popular the song is.
* danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
* energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.
* key: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.
* loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.
* mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
* speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.
* acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
* instrumentalness: Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
* liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.
* valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
* tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.
* genre: Genre of the track.

**Glimspe at the data set**
```{r}
glimpse(song)
```

**Check null values in columns**
```{r}
sapply(song, function(x) sum(is.na(x)))
```

There are no missing values in dataframe

**Check row dupicate in song column**
```{r}
which(duplicated(song$song))
```

**Delete duplicate**
```{r}
new_song <- song[!duplicated(song$song), ]
min(new_song$year)
```

**Check duplicate again in new_song**
```{r}
which(duplicated((new_song$song)))
```
## Analyze

**Correlation between variables**
```{r}
new_song1 <- new_song[ ,c(3,5,6,7,8,9,10,11,12,13,14,15,16,17)]
library(corrplot)
corrplot(cor(new_song1), method = 'square', addCoef.col = 1, number.cex = 0.55)

```

Look at the correlation graph, there are no factors effect on popularity of track.

About correlation between other factors: 

* Loudness and Energy are highly correlated: the song with high volume will make more energy
* Valence has positive correlation with danceability: cheerful songs will make it easir for people to dance
* Acousticness has negative correlation with energy and loudness. Music that solely or primarily uses instruments has low energy and volumn.

**How has music changed over years**

```{r}
mean_song <- new_song %>%
  group_by(year) %>%
  summarise_at(vars(c(popularity, danceability, energy, loudness, acousticness, instrumentalness, valence, tempo )), list(name = mean)) %>%
  rename(popularity = popularity_name, danceability = danceability_name, energy= energy_name, loudness = loudness_name, acousticness = acousticness_name, instrumentalness = instrumentalness_name, valence = valence_name, tempo = tempo_name)

ggplot(mean_song) + geom_line(aes(year, danceability, color = "danceability")) + geom_line(aes(year, energy, color = "energy")) + geom_line(aes(year, acousticness, color = "acousticness")) + geom_line(aes(year, valence, color = "valence")) + geom_line(aes(year, instrumentalness, color = "instrumentalness")) + labs(x = "Year", y =" ")
 
ggplot(mean_song) + geom_line(aes(year, loudness, color = "loudness"))
ggplot(mean_song) + geom_line(aes(year, tempo, color = "tempo"))
ggplot(mean_song) + geom_line(aes(year, popularity, color = "popularity"))

```

* From 1998-1999, songs with high energy increase and going sideway from 1999. Instead, songs are played by acoustic instrusments decrease. This is consistently with analysis above about negative correlation between energy and acoustic.
* Valence has been slowly rising from 1998-2000 and gradually decrease from there.
* Danceability hasn't changed much over years. After 2015, there is a slightly rising
* Loudness spiked from 1998 to 1999 and almosts sideway in remaining years. As relationship analysis, loudness and energy have strong correlation with each other.
* Tempo suddenly increase from 1998 to 1999, sideway from 1999-2006, continued to increase from 2006 to 2009 and moved sideways until now.
* Popularity of songs increase from 1998-1999(this is also the time energetic and loudness songs spiked), decrease gradually from 1999-2016, but between 2017 and 2018, popularity increase significantly, then dropped sharply from 2018 to 2020. 

**Top 10 artist name** 

```{r}
top_artist <- new_song %>%
  count(artist) %>%
  group_by(artist) %>%
  arrange(desc(n)) %>%
  head(10)
head(top_artist, 10)
ggplot(top_artist, aes(x= reorder(artist, -n), y = n)) + geom_bar(stat = 'identity', fill = 'darkblue')+ labs(title = 'TOP 10 ARTIST', x = 'Name of artist', y = 'Number of song') + theme(axis.text.x = element_text(angle = 30), plot.title =  element_text(hjust = 0.5))
```
The most popular artist is Rihanna.

**Top 10 track**

```{r}
top_track <- new_song %>%
  select(song, popularity) %>%
  arrange(-popularity) %>%
  head(10)
head(top_track, 10)
ggplot(top_track, aes(x = reorder(song, -popularity), y = popularity, fill = song)) + geom_bar(stat = 'identity') + labs(title = 'TOP 10 TRACK', x = 'Name of Tracks', y = 'Score of popularity') + theme(axis.text.x = element_text(angle = 90), plot.title = element_text(hjust = 0.5))
```

Sweater Weather is the song with highest popularity score, follow by it is Another Love. These songs are in the rock and pop genre, the kind of music that has a fast tempo and high energy.

## Regression linear and Hypothesis test

In this section, we will use linear regression to fit mathematical formula on the data and confirm if any variables contribute to music popularity.

```{r}
factor_effect <- lm(popularity ~ duration_ms + year + danceability + energy + key + loudness + speechiness + acousticness + instrumentalness + liveness + valence + tempo, data = new_song)
summary(factor_effect)
```

Inference for the model as a whole:

H0: There are no factors that affect popularity

H1: at least one factor affecting popularity

Since p-value = 0.2308 > 0.05. We will reject H1, accept H0. So there are no factors that affect popularity.

## Conclusion

* Audio attributes of a song don't affect a songs popularity. May be beat, lyrics, artists will make song more popular
* Tracks have high volumn will have high energy
* Tracks have less acoustic will have high energy

